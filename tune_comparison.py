"""
n_trialsì— ë”°ë¥¸ ì•™ìƒë¸” ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ìŠ¤í¬ë¦½íŠ¸

ensemble_strategy = ['voting', 'stacking'] ë‘ ê°€ì§€ ì•™ìƒë¸” ëª¨ë¸ì— ëŒ€í•´
n_trialsë¥¼ 30ë¶€í„° 450ê¹Œì§€ 30ì”© ì¦ê°€ì‹œí‚¤ë©´ì„œ íŠœë‹í•œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤.
"""

import pandas as pd
import numpy as np
import os
from typing import Dict, List
from tqdm import tqdm

from src.preprocessing import load_data, preprocess_pipeline, feature_engineering_pipeline, drop_column
from src.cv import stratified_kfold_split
from src.ensemble import train_stacking_ensemble, train_voting_ensemble, evaluate_model


def run_single_experiment(
    df: pd.DataFrame,
    ensemble_strategy: str,
    n_trials: int,
    cv_strategy: str = 'stratified_kfold',
    target_col: str = "Attrition_Binary"
) -> Dict[str, float]:
    """
    ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰: ì£¼ì–´ì§„ ensemble_strategyì™€ n_trialsë¡œ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    
    Args:
        df: ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„
        ensemble_strategy: 'voting' ë˜ëŠ” 'stacking'
        n_trials: Optuna íŠœë‹ ì‹œë„ íšŸìˆ˜
        cv_strategy: CV ì „ëµ
        target_col: íƒ€ê²Ÿ ì»¬ëŸ¼ëª…
    
    Returns:
        Dict[str, float]: í‰ê°€ ì§€í‘œ ë”•ì…”ë„ˆë¦¬
    """
    
    # í”¼ì²˜ì™€ íƒ€ê²Ÿ ë¶„ë¦¬
    features = df.drop(columns=[target_col]).columns.tolist()
    X_full = df[features]
    y_full = df[target_col]
    
    # ì „ì²´ ë°ì´í„°ë¡œ íŠœë‹í•˜ì—¬ ìµœì  íŒŒë¼ë¯¸í„° ì°¾ê¸°
    if ensemble_strategy == 'stacking':
        tuned_model, tuned_params = train_stacking_ensemble(
            X_full, y_full,
            cv_strategy=cv_strategy,
            tuning_strategy='optuna',
            n_trials=n_trials,
            return_params=True
        )
    elif ensemble_strategy == 'voting':
        tuned_model, tuned_params = train_voting_ensemble(
            X_full, y_full,
            cv_strategy=cv_strategy,
            tuning_strategy='optuna',
            n_trials=n_trials,
            return_params=True
        )
    else:
        raise ValueError(f"Unknown ensemble_strategy: {ensemble_strategy}")
    
    # CV ì„¤ì •
    if cv_strategy == 'stratified_kfold':
        folds = stratified_kfold_split(df, target_col=target_col, n_splits=5, shuffle=True, random_state=42)
    else:
        from sklearn.model_selection import train_test_split
        train_df, test_df = train_test_split(df, test_size=0.2, stratify=df[target_col], random_state=42)
        folds = [(train_df.index.tolist(), test_df.index.tolist())]
    
    # ê° í´ë“œì—ì„œ í‰ê°€ (íŠœë‹ëœ íŒŒë¼ë¯¸í„° ì‚¬ìš©)
    cv_results = []
    
    for train_idx, val_idx in folds:
        # ë°ì´í„° ë¶„í• 
        X_train = df.loc[train_idx, features]
        y_train = df.loc[train_idx, target_col]
        X_val = df.loc[val_idx, features]
        y_val = df.loc[val_idx, target_col]
        
        # ëª¨ë¸ í•™ìŠµ (íŠœë‹ëœ íŒŒë¼ë¯¸í„° ì‚¬ìš©)
        if ensemble_strategy == 'stacking':
            model = train_stacking_ensemble(
                X_train, y_train,
                cv_strategy=cv_strategy,
                tuning_strategy=None,
                best_params=tuned_params
            )
        elif ensemble_strategy == 'voting':
            model = train_voting_ensemble(
                X_train, y_train,
                cv_strategy=cv_strategy,
                tuning_strategy=None,
                best_params=tuned_params
            )
        
        # í‰ê°€ (ì¶œë ¥ ì—†ì´)
        metrics = evaluate_model(
            model, X_val, y_val,
            print_report=False
        )
        
        cv_results.append(metrics)
    
    # CV ê²°ê³¼ í‰ê·  ê³„ì‚°
    summary = {}
    for metric in cv_results[0].keys():
        values = [r[metric] for r in cv_results]
        summary[metric] = np.mean(values)
    
    return summary


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print(f"\n{'='*80}")
    print("ğŸ”¬ n_trialsì— ë”°ë¥¸ ì•™ìƒë¸” ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì‹¤í—˜")
    print(f"{'='*80}\n")
    
    # ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    print("1ï¸âƒ£ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬...")
    df = load_data()
    df = preprocess_pipeline(df)
    df = feature_engineering_pipeline(df)
    print(f"   âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {df.shape}\n")
    
    # ì‹¤í—˜ ì„¤ì •
    ensemble_strategies = ['voting', 'stacking']
    n_trials_list = list(range(30, 451, 30))  # 30, 60, 90, ..., 450
    
    print(f"2ï¸âƒ£ ì‹¤í—˜ ì„¤ì •:")
    print(f"   - ì•™ìƒë¸” ì „ëµ: {ensemble_strategies}")
    print(f"   - n_trials: {n_trials_list}")
    print(f"   - ì´ ì‹¤í—˜ ìˆ˜: {len(ensemble_strategies) * len(n_trials_list)}ê°œ\n")
    
    # ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
    results = []
    
    # ê° ì¡°í•©ì— ëŒ€í•´ ì‹¤í—˜ ì‹¤í–‰
    total_experiments = len(ensemble_strategies) * len(n_trials_list)
    experiment_num = 0
    
    for ensemble_strategy in ensemble_strategies:
        print(f"\n{'='*80}")
        print(f"ğŸ“Š ì•™ìƒë¸” ì „ëµ: {ensemble_strategy.upper()}")
        print(f"{'='*80}\n")
        
        for n_trials in tqdm(n_trials_list, desc=f"{ensemble_strategy} ì§„í–‰ ì¤‘"):
            experiment_num += 1
            print(f"\n[{experiment_num}/{total_experiments}] {ensemble_strategy} - n_trials={n_trials}")
            
            try:
                # ì‹¤í—˜ ì‹¤í–‰
                metrics = run_single_experiment(
                    df=df,
                    ensemble_strategy=ensemble_strategy,
                    n_trials=n_trials,
                    cv_strategy='stratified_kfold'
                )
                
                # ê²°ê³¼ ì €ì¥
                result_row = {
                    'ensemble_strategy': ensemble_strategy,
                    'n_trials': n_trials,
                    'accuracy': metrics['accuracy'],
                    'roc_auc': metrics['roc_auc'],
                    'pr_auc': metrics['pr_auc'],
                    'f1': metrics['f1'],
                    'recall': metrics['recall'],
                    'precision': metrics['precision']
                }
                results.append(result_row)
                
                print(f"   âœ… ì™„ë£Œ - Accuracy: {metrics['accuracy']:.4f}, F1: {metrics['f1']:.4f}, ROC-AUC: {metrics['roc_auc']:.4f}")
                
            except Exception as e:
                print(f"   âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ê²°ê³¼ì— ì¶”ê°€ (NaN ê°’ìœ¼ë¡œ)
                result_row = {
                    'ensemble_strategy': ensemble_strategy,
                    'n_trials': n_trials,
                    'accuracy': np.nan,
                    'roc_auc': np.nan,
                    'pr_auc': np.nan,
                    'f1': np.nan,
                    'recall': np.nan,
                    'precision': np.nan
                }
                results.append(result_row)
    
    # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
    results_df = pd.DataFrame(results)
    
    # ê²°ê³¼ ì €ì¥
    save_dir = 'results/tunning'
    os.makedirs(save_dir, exist_ok=True)
    save_path = os.path.join(save_dir, 'n_trials_comparison.csv')
    results_df.to_csv(save_path, index=False, encoding='utf-8-sig')
    
    print(f"\n{'='*80}")
    print("âœ… ëª¨ë“  ì‹¤í—˜ ì™„ë£Œ!")
    print(f"{'='*80}")
    print(f"\nğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {save_path}")
    print(f"\nğŸ“Š ê²°ê³¼ ìš”ì•½:")
    print(results_df.groupby('ensemble_strategy').agg({
        'accuracy': ['mean', 'std'],
        'f1': ['mean', 'std'],
        'roc_auc': ['mean', 'std']
    }))
    print(f"\n")


if __name__ == '__main__':
    main()
